{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Muscles in Action Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HumanML3D conversion rotates the models into y-axis up orientation.\n",
    "For this, we rotate around z-axis from MIA Vibe output.\n",
    "This is a valid rotation, so we do not swap anything in our preprocessing script.\n",
    "\n",
    "Note, that we center our model on the origin in the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "\n",
    "np.bool = np.bool_\n",
    "np.int = np.int_\n",
    "np.float = np.float_\n",
    "np.complex = np.complex_\n",
    "np.object = np.object_\n",
    "np.unicode = np.unicode_\n",
    "np.str = np.str_\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from smplx.body_models import SMPLH\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R, Slerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_angle_to_quaternion(axis_angle):\n",
    "    \"\"\"Convert axis-angle to quaternion.\"\"\"\n",
    "    rotation = R.from_rotvec(axis_angle)\n",
    "    return rotation.as_quat()\n",
    "\n",
    "\n",
    "def quaternion_to_axis_angle(quaternion):\n",
    "    \"\"\"Convert quaternion to axis-angle.\"\"\"\n",
    "    rotation = R.from_quat(quaternion)\n",
    "    return rotation.as_rotvec()\n",
    "\n",
    "\n",
    "def slerp(t, q0, q1):\n",
    "    \"\"\"Spherical linear interpolation (slerp) of quaternions.\"\"\"\n",
    "    rotations = R.from_quat([q0, q1])\n",
    "    slerp = Slerp([0, 1], rotations)\n",
    "    return slerp(t).as_quat()\n",
    "\n",
    "\n",
    "def interpolate_poses(pose1, pose2, n_interpolations=1):\n",
    "    \"\"\"Interpolate between two sets of pose parameters (axis-angles).\"\"\"\n",
    "    interpolated_poses = []\n",
    "    for i in range(1, n_interpolations + 1):\n",
    "        t = i / (n_interpolations + 1)\n",
    "        interpolated_pose = []\n",
    "        for j in range(len(pose1) // 3):\n",
    "            aa1 = pose1[3 * j : 3 * j + 3]\n",
    "            aa2 = pose2[3 * j : 3 * j + 3]\n",
    "            q1 = axis_angle_to_quaternion(aa1)\n",
    "            q2 = axis_angle_to_quaternion(aa2)\n",
    "            qi = slerp(t, q1, q2)\n",
    "            interpolated_pose.extend(quaternion_to_axis_angle(qi))\n",
    "        interpolated_poses.append(interpolated_pose)\n",
    "    return interpolated_poses\n",
    "\n",
    "\n",
    "def interpolate_batch(batch, interpolation_func, n_interpolations=1):\n",
    "    \"\"\"Interpolate each pair of consecutive samples in a batch.\"\"\"\n",
    "    interpolated_batch = []\n",
    "    for i in range(len(batch) - 1):\n",
    "        interpolated_batch.append(batch[i])\n",
    "        interpolated_intermediates = interpolation_func(batch[i], batch[i + 1], n_interpolations)\n",
    "        interpolated_batch.extend(interpolated_intermediates)\n",
    "    interpolated_batch.append(batch[-1])\n",
    "    return np.array(interpolated_batch)\n",
    "\n",
    "\n",
    "def interpolate_linear(param1, param2, n_interpolations=1):\n",
    "    \"\"\"Linear interpolation of parameters.\"\"\"\n",
    "    interpolated_params = []\n",
    "    for i in range(1, n_interpolations + 1):\n",
    "        t = i / (n_interpolations + 1)\n",
    "        interpolated_param = (1 - t) * param1 + t * param2\n",
    "        interpolated_params.append(interpolated_param)\n",
    "    return interpolated_params\n",
    "\n",
    "\n",
    "def convert_pare_to_full_img_cam(pare_cam, bbox_width, bbox_height, bbox_center, img_w, img_h, focal_length):\n",
    "    # From https://github.com/mchiquier/musclesinaction/tree/main\n",
    "    # Converts weak perspective camera estimated by PARE in\n",
    "    # bbox coords to perspective camera in full image coordinates\n",
    "    # from https://arxiv.org/pdf/2009.06549.pdf\n",
    "    s, tx, ty = pare_cam[:, 0], pare_cam[:, 1], pare_cam[:, 2]\n",
    "    res = 224\n",
    "    tz = 2 * focal_length / (res * s)\n",
    "    # pdb.set_trace()\n",
    "    cx = 2 * (bbox_center[:, 0] - (img_w / 2.0)) / (s * bbox_width)\n",
    "    cy = 2 * (bbox_center[:, 1] - (img_h / 2.0)) / (s * bbox_height)\n",
    "\n",
    "    cam_t = np.stack([tx + cx, ty + cy, tz], axis=-1)\n",
    "\n",
    "    return cam_t\n",
    "\n",
    "\n",
    "def get_leaf_directories(root_dir):\n",
    "    leaf_directories = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if not dirnames:\n",
    "            leaf_directories.append(dirpath)\n",
    "    return leaf_directories\n",
    "\n",
    "\n",
    "def mia_to_smpl_body(pose_dir, bm):\n",
    "    pose_np = np.load(opj(pose_dir, \"pose.npy\"))\n",
    "    betas_np = np.load(opj(pose_dir, \"betas.npy\"))\n",
    "    predcam_np = np.load(opj(pose_dir, \"predcam.npy\"))\n",
    "    bboxes_np = np.load(opj(pose_dir, \"bboxes.npy\"))\n",
    "\n",
    "    bbox_center = np.stack((bboxes_np[:, 0] + bboxes_np[:, 2] / 2, bboxes_np[:, 1] + bboxes_np[:, 3] / 2), axis=-1)\n",
    "    bbox_width = bboxes_np[:, 2]\n",
    "    bbox_height = bboxes_np[:, 3]\n",
    "\n",
    "    # These parameters result from the MIA settings, extracted from https://github.com/mchiquier/musclesinaction/tree/main\n",
    "    transl_np = convert_pare_to_full_img_cam(\n",
    "        pare_cam=predcam_np,\n",
    "        bbox_width=bbox_width,\n",
    "        bbox_height=bbox_height,\n",
    "        bbox_center=bbox_center,\n",
    "        img_w=1920,\n",
    "        img_h=1080,\n",
    "        focal_length=5000,\n",
    "    )\n",
    "\n",
    "    transl_np = transl_np - transl_np[0]\n",
    "\n",
    "    # Vibe depth estimation is not really good, but in Muscles in Action most actions (roughly) happen in a plane\n",
    "    # So we can set the z translation to 0\n",
    "    transl_np[:, 2] = 0\n",
    "\n",
    "    # Interpolate in bnetween to get 59 pose samples (20 fps instead of 10 fps)\n",
    "    pose_np_inter = interpolate_batch(pose_np, interpolate_poses, n_interpolations=1)\n",
    "    betas_np_inter = interpolate_batch(betas_np, interpolate_linear, n_interpolations=1)\n",
    "    transl_np_inter = interpolate_batch(transl_np, interpolate_linear, n_interpolations=1)\n",
    "\n",
    "    # Ensure the shapes are correct for the SMPL model\n",
    "    assert pose_np.shape[1] == 72, \"Each pose should have 72 parameters (24 joints * 3 rotations).\"\n",
    "    assert betas_np.shape[1] == 10, \"Each betas should have 10 parameters (shape coefficients).\"\n",
    "\n",
    "    pose_tensor = torch.tensor(pose_np_inter, dtype=torch.float32).cuda()\n",
    "    betas_tensor = torch.tensor(betas_np_inter, dtype=torch.float32).cuda()\n",
    "    transl_tensor = torch.tensor(transl_np_inter, dtype=torch.float32).cuda()\n",
    "\n",
    "    # We rotate the model into the same orientation as the AMASS samples\n",
    "    # Since HumanML3D is made for AMASS samples.\n",
    "\n",
    "    # Assuming pose_tensor is already defined with shape (30, 3)\n",
    "    orig_rotation = pose_tensor[:, :3].cpu().numpy()  # Shape (30,3), axis angle representation\n",
    "\n",
    "    # Rotation to be applied\n",
    "    rotation_matrix = np.array([[-1.0, 0.0, 0.0], [0.0, -1, 0], [0.0, 0, 1]])\n",
    "\n",
    "    # Convert axis-angle to rotation matrices\n",
    "    orig_rot_matrices = R.from_rotvec(orig_rotation).as_matrix()  # Shape (30, 3, 3)\n",
    "\n",
    "    # Apply the new rotation matrix\n",
    "    new_rot_matrices = np.einsum(\"ij,kjl->kil\", rotation_matrix, orig_rot_matrices)  # Shape (30, 3, 3)\n",
    "\n",
    "    # Convert back to axis-angle representation if needed\n",
    "    new_orientation = R.from_matrix(new_rot_matrices).as_rotvec()  # Shape (30, 3)\n",
    "\n",
    "    new_orientation = torch.tensor(new_orientation).float().cuda()\n",
    "\n",
    "    # Create SMPLH body model\n",
    "    # Assume zero rotation for hands\n",
    "    # 15 joints per hand * 3 rotations = 45\n",
    "    left_hand_pose = torch.zeros((pose_tensor.shape[0], 45)).cuda()\n",
    "    right_hand_pose = torch.zeros((pose_tensor.shape[0], 45)).cuda()\n",
    "\n",
    "    body = bm(\n",
    "        betas=betas_tensor,\n",
    "        body_pose=pose_tensor[:, 3:66],\n",
    "        left_hand_pose=left_hand_pose,\n",
    "        right_hand_pose=right_hand_pose,\n",
    "        global_orient=new_orientation,\n",
    "        transl=transl_tensor,\n",
    "    )\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def mia_to_pose(pose_dir, bm, save_path):\n",
    "    body = mia_to_smpl_body(pose_dir, bm)\n",
    "\n",
    "    pose_seq_np = body.joints.detach().cpu().numpy()\n",
    "\n",
    "    # Since we use the smplx package, we have a different implementation to the original HumanML3D scripts.\n",
    "    # HML3D only uses the 52 kinematic tree joints from smplh, but smplx package implementation adds 21 extra joints from surface nodes.\n",
    "    # such as eyes, ears, hand and feet positions on the skin.\n",
    "    # We remove these extra joints to match the original implementation.\n",
    "    pose_seq_np = pose_seq_np[:, :52]\n",
    "\n",
    "    np.save(save_path, pose_seq_np)\n",
    "\n",
    "\n",
    "def swap_left_right(data):\n",
    "    assert len(data.shape) == 3 and data.shape[-1] == 3\n",
    "    data = data.copy()\n",
    "    data[..., 0] *= -1\n",
    "    right_chain = [2, 5, 8, 11, 14, 17, 19, 21]\n",
    "    left_chain = [1, 4, 7, 10, 13, 16, 18, 20]\n",
    "    left_hand_chain = [22, 23, 24, 34, 35, 36, 25, 26, 27, 31, 32, 33, 28, 29, 30]\n",
    "    right_hand_chain = [43, 44, 45, 46, 47, 48, 40, 41, 42, 37, 38, 39, 49, 50, 51]\n",
    "    tmp = data[:, right_chain]\n",
    "    data[:, right_chain] = data[:, left_chain]\n",
    "    data[:, left_chain] = tmp\n",
    "    if data.shape[1] > 24:\n",
    "        tmp = data[:, right_hand_chain]\n",
    "        data[:, right_hand_chain] = data[:, left_hand_chain]\n",
    "        data[:, left_hand_chain] = tmp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "smpl_h_path = \"./body_models/smpl/SMPLH_NEUTRAL_AMASS_MERGED.pkl\"\n",
    "\n",
    "# Each sample in MIA has 30 framesa at 10 fps. We convert to 20 fps by interpolating intermediate frames\n",
    "# Since we only interpolate between two frames we end up with 59 frames as result.\n",
    "bm = SMPLH(model_path=smpl_h_path, num_betas=10, use_pca=False, batch_size=59).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/cvhci/data/activity/MIADatasetOfficial/val/Subject0/HookPunch/5', '/cvhci/data/activity/MIADatasetOfficial/val/Subject0/HookPunch/55', '/cvhci/data/activity/MIADatasetOfficial/val/Subject0/HookPunch/109']\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/cvhci/data/activity/MIADatasetOfficial\"\n",
    "pose_data_dir = \"./MIAHML3D/pose_data\"\n",
    "\n",
    "sample_dirs = get_leaf_directories(root_dir)\n",
    "\n",
    "print(sample_dirs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few hours for all datasets, here we take one dataset as an example\n",
    "\n",
    "To accelerate the process, you could run multiple scripts like this at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24340 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create the directories if they do not exist\u001b[39;00m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dire, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmia_to_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 162\u001b[0m, in \u001b[0;36mmia_to_pose\u001b[0;34m(pose_dir, bm, save_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmia_to_pose\u001b[39m(pose_dir, bm, save_path):\n\u001b[0;32m--> 162\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[43mmia_to_smpl_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     pose_seq_np \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mjoints\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    166\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(save_path, pose_seq_np)\n",
      "Cell \u001b[0;32mIn[2], line 149\u001b[0m, in \u001b[0;36mmia_to_smpl_body\u001b[0;34m(pose_dir, bm)\u001b[0m\n\u001b[1;32m    146\u001b[0m left_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((pose_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m45\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    147\u001b[0m right_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((pose_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m45\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 149\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m(\n\u001b[1;32m    150\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas_tensor,\n\u001b[1;32m    151\u001b[0m     body_pose\u001b[38;5;241m=\u001b[39mpose_tensor[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m66\u001b[39m],\n\u001b[1;32m    152\u001b[0m     left_hand_pose\u001b[38;5;241m=\u001b[39mleft_hand_pose,\n\u001b[1;32m    153\u001b[0m     right_hand_pose\u001b[38;5;241m=\u001b[39mright_hand_pose,\n\u001b[1;32m    154\u001b[0m     global_orient\u001b[38;5;241m=\u001b[39mnew_orientation,\n\u001b[1;32m    155\u001b[0m     transl\u001b[38;5;241m=\u001b[39mtransl_tensor,\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m body\n",
      "Cell \u001b[0;32mIn[2], line 149\u001b[0m, in \u001b[0;36mmia_to_smpl_body\u001b[0;34m(pose_dir, bm)\u001b[0m\n\u001b[1;32m    146\u001b[0m left_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((pose_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m45\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    147\u001b[0m right_hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((pose_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m45\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 149\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m(\n\u001b[1;32m    150\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas_tensor,\n\u001b[1;32m    151\u001b[0m     body_pose\u001b[38;5;241m=\u001b[39mpose_tensor[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m66\u001b[39m],\n\u001b[1;32m    152\u001b[0m     left_hand_pose\u001b[38;5;241m=\u001b[39mleft_hand_pose,\n\u001b[1;32m    153\u001b[0m     right_hand_pose\u001b[38;5;241m=\u001b[39mright_hand_pose,\n\u001b[1;32m    154\u001b[0m     global_orient\u001b[38;5;241m=\u001b[39mnew_orientation,\n\u001b[1;32m    155\u001b[0m     transl\u001b[38;5;241m=\u001b[39mtransl_tensor,\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m body\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/hm3d/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hm3d/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in tqdm(sample_dirs):\n",
    "    save_path = path.replace(root_dir, pose_data_dir)\n",
    "    save_path = save_path + \".npy\"\n",
    "\n",
    "    # Get the directory path\n",
    "    dire = os.path.dirname(save_path)\n",
    "\n",
    "    # Create the directories if they do not exist\n",
    "    os.makedirs(dire, exist_ok=True)\n",
    "\n",
    "    mia_to_pose(path, bm, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will extract poses from **AMASS** dataset, and put them under directory **\"./pose_data\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source data from **HumanAct12** is already included in **\"./pose_data\"** in this repository. You need to **unzip** it right in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment, Mirror and Relocate Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data_dir = \"MIAHML3D/pose_data\"\n",
    "save_dir = \"MIAHML3D/joints\"\n",
    "\n",
    "# Get a list of all .npy files in the directory tree\n",
    "pose_samples = glob.glob(os.path.join(pose_data_dir, \"**\", \"*.npy\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MIAHML3D/pose_data/train/Subject3/HighKick/875.npy'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24340/24340 [14:49<00:00, 27.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for pose_path in tqdm(pose_samples):\n",
    "    dire, file = os.path.split(pose_path)\n",
    "\n",
    "    dire = dire.replace(pose_data_dir, save_dir)\n",
    "\n",
    "    save_filepath = opj(dire, file)\n",
    "    save_filepath_m = opj(dire, \"M_\" + file)\n",
    "\n",
    "    # Create the directories if they do not exist\n",
    "    os.makedirs(dire, exist_ok=True)\n",
    "\n",
    "    data = np.load(pose_path)\n",
    "    data_m = swap_left_right(data)\n",
    "\n",
    "    np.save(save_filepath, data)\n",
    "    np.save(save_filepath_m, data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hm3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
